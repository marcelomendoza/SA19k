{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incoming-arlington",
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data import Data\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from model import *\n",
    "from torch.optim.lr_scheduler import ExponentialLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stopped-medication",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "\n",
    "    def __init__(self, learning_rate=0.0005, ent_vec_dim=200, rel_vec_dim=200, \n",
    "                 num_iterations=500, batch_size=128, decay_rate=0., cuda=False, \n",
    "                 input_dropout=0.3, hidden_dropout1=0.4, hidden_dropout2=0.5,\n",
    "                 label_smoothing=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.ent_vec_dim = ent_vec_dim\n",
    "        self.rel_vec_dim = rel_vec_dim\n",
    "        self.num_iterations = num_iterations\n",
    "        self.batch_size = batch_size\n",
    "        self.decay_rate = decay_rate\n",
    "        self.label_smoothing = label_smoothing\n",
    "        self.cuda = cuda\n",
    "        self.kwargs = {\"input_dropout\": input_dropout, \"hidden_dropout1\": hidden_dropout1,\n",
    "                       \"hidden_dropout2\": hidden_dropout2}\n",
    "        \n",
    "    def get_data_idxs(self, data):\n",
    "        data_idxs = [(self.entity_idxs[data[i][0]], self.relation_idxs[data[i][1]], \\\n",
    "                      self.entity_idxs[data[i][2]]) for i in range(len(data))]\n",
    "        return data_idxs\n",
    "    \n",
    "    def get_er_vocab(self, data):\n",
    "        er_vocab = defaultdict(list)\n",
    "        for triple in data:\n",
    "            er_vocab[(triple[0], triple[1])].append(triple[2])\n",
    "        return er_vocab\n",
    "\n",
    "    def get_batch(self, er_vocab, er_vocab_pairs, idx):\n",
    "        batch = er_vocab_pairs[idx:idx+self.batch_size]\n",
    "        targets = np.zeros((len(batch), len(d.entities)))\n",
    "        for idx, pair in enumerate(batch):\n",
    "            targets[idx, er_vocab[pair]] = 1.\n",
    "        targets = torch.FloatTensor(targets)\n",
    "        if self.cuda:\n",
    "            targets = targets.cuda()\n",
    "        return np.array(batch), targets\n",
    "\n",
    "    \n",
    "    def evaluate(self, model, data):\n",
    "        hits = []\n",
    "        ranks = []\n",
    "        for i in range(10):\n",
    "            hits.append([])\n",
    "\n",
    "        test_data_idxs = self.get_data_idxs(data)\n",
    "        er_vocab = self.get_er_vocab(self.get_data_idxs(d.data))\n",
    "\n",
    "        print(\"Number of data points: %d\" % len(test_data_idxs))\n",
    "        \n",
    "        for i in range(0, len(test_data_idxs), self.batch_size):\n",
    "            data_batch, _ = self.get_batch(er_vocab, test_data_idxs, i)\n",
    "            e1_idx = torch.tensor(data_batch[:,0])\n",
    "            r_idx = torch.tensor(data_batch[:,1])\n",
    "            e2_idx = torch.tensor(data_batch[:,2])\n",
    "            if self.cuda:\n",
    "                e1_idx = e1_idx.cuda()\n",
    "                r_idx = r_idx.cuda()\n",
    "                e2_idx = e2_idx.cuda()\n",
    "            predictions = model.forward(e1_idx, r_idx)\n",
    "\n",
    "            for j in range(data_batch.shape[0]):\n",
    "                filt = er_vocab[(data_batch[j][0], data_batch[j][1])] # target eidx\n",
    "                target_value = predictions[j,e2_idx[j]].item()\n",
    "                predictions[j, filt] = 0.0\n",
    "                predictions[j, e2_idx[j]] = target_value\n",
    "\n",
    "            sort_values, sort_idxs = torch.sort(predictions, dim=1, descending=True)\n",
    "\n",
    "            sort_idxs = sort_idxs.cpu().numpy()\n",
    "            for j in range(data_batch.shape[0]):\n",
    "                rank = np.where(sort_idxs[j]==e2_idx[j].item())[0][0]\n",
    "                ranks.append(rank+1)\n",
    "\n",
    "                for hits_level in range(10):\n",
    "                    if rank <= hits_level:\n",
    "                        hits[hits_level].append(1.0)\n",
    "                    else:\n",
    "                        hits[hits_level].append(0.0)\n",
    "\n",
    "        print('Hits @10: {0}'.format(np.mean(hits[9])))\n",
    "        print('Hits @3: {0}'.format(np.mean(hits[2])))\n",
    "        print('Hits @1: {0}'.format(np.mean(hits[0])))\n",
    "        print('Mean rank: {0}'.format(np.mean(ranks)))\n",
    "        print('Mean reciprocal rank: {0}'.format(np.mean(1./np.array(ranks))))\n",
    "    \n",
    "    def train_and_eval(self):\n",
    "        print(\"Training the TuckER model...\")\n",
    "        self.entity_idxs = {d.entities[i]:i for i in range(len(d.entities))}\n",
    "        self.relation_idxs = {d.relations[i]:i for i in range(len(d.relations))}\n",
    "\n",
    "        train_data_idxs = self.get_data_idxs(d.train_data)\n",
    "        print(\"Number of training data points: %d\" % len(train_data_idxs))\n",
    "\n",
    "        model = TuckER(d, self.ent_vec_dim, self.rel_vec_dim, **self.kwargs)\n",
    "        if self.cuda:\n",
    "            model.cuda()\n",
    "        model.init()\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=self.learning_rate)\n",
    "        if self.decay_rate:\n",
    "            scheduler = ExponentialLR(opt, self.decay_rate)\n",
    "\n",
    "        er_vocab = self.get_er_vocab(train_data_idxs)\n",
    "        er_vocab_pairs = list(er_vocab.keys())\n",
    "\n",
    "        print(\"Starting training...\")\n",
    "        for it in range(1, self.num_iterations+1):\n",
    "            start_train = time.time()\n",
    "            model.train()    \n",
    "            losses = []\n",
    "            np.random.shuffle(er_vocab_pairs)\n",
    "            for j in range(0, len(er_vocab_pairs), self.batch_size):\n",
    "                data_batch, targets = self.get_batch(er_vocab, er_vocab_pairs, j)\n",
    "                opt.zero_grad()\n",
    "                e1_idx = torch.tensor(data_batch[:,0])\n",
    "                r_idx = torch.tensor(data_batch[:,1])  \n",
    "                if self.cuda:\n",
    "                    e1_idx = e1_idx.cuda()\n",
    "                    r_idx = r_idx.cuda()\n",
    "                predictions = model.forward(e1_idx, r_idx)\n",
    "                if self.label_smoothing:\n",
    "                    targets = ((1.0-self.label_smoothing)*targets) + (1.0/targets.size(1)) # as SACN\n",
    "                loss = model.loss(predictions, targets)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                losses.append(loss.item())\n",
    "            if self.decay_rate:\n",
    "                scheduler.step()\n",
    "            print(it)\n",
    "            print(time.time()-start_train)    \n",
    "            print(np.mean(losses))\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                print(\"Validation:\")\n",
    "                self.evaluate(model, d.valid_data)\n",
    "                if not it%2:\n",
    "                    print(\"Test:\")\n",
    "                    start_test = time.time()\n",
    "                    self.evaluate(model, d.test_data)\n",
    "                    print(time.time()-start_test)\n",
    "        \n",
    "        return model\n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "living-hydrogen",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 100 # 200\n",
    "batch_size = 128\n",
    "lr = 0.005 # 0.0005, 0.005, 0.01\n",
    "dr = 1.0  \n",
    "edim = 200 # 100, 200, 500, 1000\n",
    "rdim = 10  # 10, 20, 30\n",
    "cuda = True\n",
    "input_dropout   = 0.5   # 0.2, 0.3, 0.4\n",
    "hidden_dropout1 = 0.2   # 0.2, 0.3, 0.4\n",
    "hidden_dropout2 = 0.2   # 0.2, 0.3, 0.4, 0.5\n",
    "label_smoothing = 0.1\n",
    "\n",
    "data_dir = \"/\" % dataset\n",
    "torch.backends.cudnn.deterministic = True \n",
    "seed = 10\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available:\n",
    "    torch.cuda.manual_seed_all(seed) \n",
    "d = Data(data_dir=data_dir, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-boston",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(num_iterations=num_iterations, batch_size=batch_size, learning_rate=lr, \n",
    "                            decay_rate=dr, ent_vec_dim=edim, rel_vec_dim=rdim, cuda=cuda,\n",
    "                            input_dropout=input_dropout, hidden_dropout1=hidden_dropout1, \n",
    "                            hidden_dropout2=hidden_dropout2, label_smoothing=label_smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-endorsement",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = experiment.train_and_eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
